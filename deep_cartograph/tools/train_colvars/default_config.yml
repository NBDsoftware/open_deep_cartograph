# cvs:                           List of Collective Variables to calculate: pca, ae, tica, deep_tica or htica
# common:                        Common settings for the Collective Variables
#   dimension:                    (int) Number of dimensions to calculate
#   num_subspaces:                (int) Number of subspaces to calculate (only for htica) - increase to reduce the memory usage
#   subspaces_dimension:          (int) Dimension of the subspaces to calculate (only for htica) - reduce to reduce the memory usage
#   features_normalization:       (str) Normalization of the input features, e.g. 'mean_std', 'min_max', null
#   input_colvars:              Settings for the input colvars file reading with the time series of the input features
#     start:                      (int) Start index to read the input features (df.iloc[start:stop:stride])
#     stop:                       (int) Stop index to read the input features (df.iloc[start:stop:stride])
#     stride:                     (int) Stride to read the input features (df.iloc[start:stop:stride])
#   architecture:                 Settings for the architecture of the Collective Variables
#     hidden_layers:              (list) Fully connected hidden layers between the input and latent space, e.g. [15, 15]
#     lag_time:                   (int) Lag time for the Collective Variables
#   training:                    Settings for the training of the Collective Variables (when applicable)
#     general:                    General settings for the training
#       max_tries:                    (int) Maximum number of tries for the training
#       seed:                         (int) Seed for the PyTorch random number generator
#       lengths:                      (list) Lengths of the training and validation sets, e.g. [0.8, 0.2]
#       batch_size:                   (int) Batch size for the training
#       max_epochs:                   (int) Maximum number of epochs for the training
#       dropout:                      (float) Dropout rate for the training
#       shuffle:                      (bool) Shuffle the data before training 
#       check_val_every_n_epoch:      (int) Do a validation check every n epochs
#       save_check_every_n_epoch:     (int) Save the model every n epochs
#     early_stopping:              Settings for the early stopping
#       patience:                     (int) Patience for the early stopping, i.e., the number of validation checks with no improvement after which training will be stopped
#       min_delta:                    (float) Minimum change in the loss function to consider it an improvement
#     optimizer:                   Settings for the optimizer
#       name:                         (str) Name of the optimizer
#       kwargs:                       (dict) Keyword arguments for the optimizer
#     save_loss:                    (bool) Wether to save the training and validation losses after training 
#     plot_loss:                    (bool) Wether to plot the loss after training

cvs: ['pca', 'ae', 'tica', 'deep_tica']
common:
  dimension: 1
  features_normalization: 'mean_std' # ['mean_std', 'min_max', null]
  input_colvars: 
    start: 0
    stop: null
    stride: 1 
  architecture:
    hidden_layers: [15, 15]
    lag_time: 10
  training:
    general:
      max_tries: 10
      seed: 42
      lengths: [0.8, 0.2]
      batch_size: 32
      max_epochs: 1000
      dropout: 0.1
      shuffle: False
      random_split: False
      check_val_every_n_epoch: 10
      save_check_every_n_epoch: 10
    early_stopping:
      patience: 2
      min_delta: 0.00001
    optimizer:
      name: Adam
      kwargs: 
        lr: 0.01
        weight_decay: 0.0
    save_loss: True
    plot_loss: True
ae:
  training:
    general: 
      shuffle: True
      random_split: True
    optimizer:
      kwargs: 
        lr: 0.01
htica:
  num_subspaces: 10
  subspaces_dimension: 5

# figures:                          Settings for additional figures
#   fes:                              Settings for the Free Energy Surface calculation
#     compute:                          (bool) Calculate the Free Energy Surface
#     save:                             (bool) Save the calculated Free Energy Surface in .npy files (otherwise it just plots 1D or 2D FES)
#     temperature:                      (int) Temperature in Kelvin
#     bandwidth:                        (float) Bandwidth for the Kernel Density Estimation of the Free Energy Surface
#     num_bins:                         (int) Number of bins for the Kernel Density Estimation of the Free Energy Surface
#     num_blocks:                       (int) Number of blocks for the standard error calculation of the Free Energy Surface
#     max_fes:                          (float) Maximum value for the Free Energy Surface (above which the value is set to NaN)
#   traj_projection:                Settings for the Projected Trajectory
#     plot:                             (bool) Plot the Projected Trajectory
#     num_bins:                         (int) Number of bins for the Kernel Density Estimation of the Projected Trajectory
#     bandwidth:                        (float) Bandwidth for the Kernel Density Estimation of the Projected Trajectory
#     alpha:                            (float) Transparency of the points in the Projected Trajectory
#     cmap:                             (str) Colormap for the Projected Trajectory
#     use_legend:                        (bool) Use a legend in the Projected Clustered Trajectory plot
#     marker_size:                      (int) Size of the markers in the Projected Trajectory

figures:
  fes:
    compute: True  
    save: True
    temperature: 300
    bandwidth: 0.01
    num_bins: 100
    num_blocks: 1
    max_fes: 40
  traj_projection:
    plot: True
    num_bins: 100
    bandwidth: 0.25
    alpha: 0.6
    cmap: turbo
    use_legend: True
    marker_size: 12

# clustering:                        Settings for the clustering
#   run:                              (bool) Whether to run the clustering or not
#   algorithm:                        (str: kmeans, hdbscan, hierarchical) Clustering algorithm to use
#   opt_num_clusters:                 (bool) Whether to search for the optimal number of clusters inside the search_interval or not (only for hierarchical and kmeans)
#   search_interval:                  (list) Range of number of clusters to search for the optimal number of clusters (only for hierarchical and kmeans)
#   num_clusters:                     (int) Number of clusters to use (only for hierarchical and kmeans and if opt_num_clusters is false)
#   linkage:                          (str) Linkage criterion to use ('ward', 'single', 'average', 'complete') (only for hierarchical)
#   n_init:                           (int) Number of times the k-means algorithm is run with different centroid seeds (only for kmeans)
#   min_cluster_size:                 (int) Minimum number of samples in a group for that group to be considered a cluster; groupings smaller than this size will be left as noise (only for hdbscan)
#   max_cluster_size:                 (int) Maximum number of samples in a group for that group to be considered a cluster; If null, there is no limit (only for hdbscan)
#   min_samples:                      (int) Number of samples in a neighborhood for a point to be considered as a core point (only for hdbscan)
#   cluster_selection_epsilon:        (float) A distance threshold. Clusters below this value will be merged (only for hdbscan)
#   cluster_selection_method:         (str) Method to select the number of clusters ('eom', 'leaf') (only for hdbscan)

#   Note that:
#     min_cluster_size should be set to the smallest size grouping that you wish to consider a cluster.
#     the larger the value of min_samples you provide, the more conservative the clustering (more points will be declared as noise) and clusters will be restricted to progressively more dense areas

clustering:                       
  run: True                        
  algorithm: hdbscan               
  opt_num_clusters: True           
  search_interval: [3, 8]         
  num_clusters: 3                  
  linkage: complete                
  n_init: 20                    
  min_cluster_size: 5               # Increase if some of the clusters are too small
  max_cluster_size: null          
  min_samples: 3                    # Increase to obtain more conservative clustering restricted to more dense areas
  cluster_selection_epsilon: 0      # Increase to merge more clusters
  cluster_selection_method: eom     # Choose 'leaf' to obtain many small homogeneous clusters